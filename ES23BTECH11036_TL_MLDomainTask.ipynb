{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SohamMashetty/Tl_ML_Task/blob/main/ES23BTECH11036_TL_MLDomainTask.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efisdUBMQtBx"
      },
      "source": [
        "# `**Tinkerers Lab ML domain Task**`\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a92rYoXwQbd0"
      },
      "source": [
        "# Problem Statement\n",
        "You are provided with a dataset containing a collection of images and corresponding captions that describe\n",
        "the images. Your task is to develop a machine learning model that can generate captions for the images\n",
        "in the dataset. The model should produce human-like descriptions of the images, capturing the key details\n",
        "and context within each image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pugOsIVmU6GJ"
      },
      "source": [
        "## Importing required libraries\n",
        "*   The BertTokenizer is used to tokenize the captions. Tokenization converts the text captions into a format that can be processed by the model, specifically into input IDs. BERT's\n",
        "\n",
        "*   Dataset and DataLoader are used to create and manage the data pipeline. In the model I created a class called FlickrDset that handles the loading and preprocessing of images and captions.\n",
        "\n",
        "*   DataLoader is used to create iterators for batching and shuffling the data during training and evaluation.\n",
        "\n",
        "*   torch.nn.functional provides various functions for neural network operations, for example I have used the cross_entropy function inorder to compute the loss between the predicted captions and the ground truth captions.\n",
        "\n",
        "*   torch.nn is used to build neural network layers and models. nn.Module is the base class for all neural network modules in PyTorch. I have used layers such as nn.Embedding, nn.Linear, and nn.TransformerDecoder in the model.\n",
        "\n",
        "*   PyTorch is the core library which is generally used for building and training neural networks. It provides tensor operations, automatic differentiation, and other utilities needed for deep learning tasks.\n",
        "\n",
        "*   feature_extraction is used to create a feature extractor from a CNN (ViT in this case). This allowed me to use the intermediate features from the pre-trained model as inputs to the caption generator.\n",
        "\n",
        "*   torchvision is used for tasks related to computer vision.\n",
        "\n",
        "*   matplotlib.pyplot is used for visualizing images and plots. I have used to display images along with the generated captions\n",
        "\n",
        "*   I used tqdm to create progress bars for loops. This helps in tracking the progress of training and evaluation loops.Since the taining takes approximately 7hrs and evaluation approximately 2 hrs\n",
        "\n",
        "*   PIL (Python Imaging Library) handles image loading and manipulation.I have used to open images and convert them to the appropriate format for preprocessing.\n",
        "\n",
        "*   I used os for handling file paths and directory operations. This was needed to load the data from the drive in this particular case.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YnXgYAHxP3l-"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "from torchvision.models import feature_extraction\n",
        "import torchvision\n",
        "\n",
        "import matplotlib.pyplot as lyb\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRi1riK8YcIP"
      },
      "source": [
        "## Setting Random Seeds for Reproducibility\n",
        "This code block sets the random seeds for NumPy and PyTorch to ensure reproducibility of the results. By fixing the random seed, the same sequence of random numbers is generated each time, which helps in obtaining consistent and comparable results across different runs of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWMny8qLXO6V"
      },
      "outputs": [],
      "source": [
        "SEED = 123\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsCg8_5tZ5eC"
      },
      "source": [
        "## Setting up hyperparameters, Giving ratios to split data and Setting up directory paths\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "I1xSUeOiXzTA",
        "outputId": "b17ed8ca-7b16-41b7-c51b-06e5cfdd4b1e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cpu'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "DATA_DIR = \"/content/drive/MyDrive/image_captions\"\n",
        "IMAGES_DIR = os.path.join(DATA_DIR,'Images')\n",
        "\n",
        "SPLIT_RATIO = {\n",
        "     'train' : 0.6,\n",
        "     'val' : 0.2,\n",
        "     'test' : 0.2,\n",
        " }\n",
        "N_EPOCHS = 10\n",
        "BATCH_SIZE = 128\n",
        "EMBED_SIZE = 768\n",
        "LEARNING_RATE = 1e-4\n",
        "NHEAD=1\n",
        "NUM_LAYERS=1\n",
        "BEST_MODEL_FILEPATH = \"best_model.pt\"\n",
        "EARLY_STOPPING_STEP_SIZE = 5\n",
        "\n",
        "\n",
        "\n",
        "WEIGHTS = torchvision.models.ViT_B_16_Weights.DEFAULT\n",
        "BASE_MODEL = torchvision.models.vit_b_16\n",
        "FEAT_LAYER = \"getitem_5\"\n",
        "FEAT_DIMS = 768\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "DEVICE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_75f_JcbPMs"
      },
      "source": [
        "## Loading Caption Data\n",
        "This cell reads and parses the captions dataset, splitting it into image filenames and corresponding captions.The function load_data reads the file captions.txt . Each line is stripped of extra quotes and split into image filename and caption. The resulting lists of image filenames and captions are then zipped and returned. It ensures the dataset is correctly loaded and the number of images matches the number of captions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vfw0vrZXaOK6",
        "outputId": "a5f97e7e-7114-46a2-e7ee-644beacc46f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 40455 images and captions\n"
          ]
        }
      ],
      "source": [
        "def load_data(fp):\n",
        "    with open(fp, mode='r') as f:\n",
        "        data = [row.strip().replace('\"', '').split(',', 1) for idx, row in enumerate(f) if idx > 0]\n",
        "    return zip(*data)\n",
        "\n",
        "data_filepath = os.path.join(DATA_DIR, 'captions.txt')\n",
        "images, captions = load_data(data_filepath)\n",
        "assert len(images) == len(captions)\n",
        "\n",
        "n_data = len(images)\n",
        "print(f\"There are {n_data} images and captions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7L8k2n4cVSk"
      },
      "source": [
        "## Tokenizing Captions and Determining Maximum Sequence Length\n",
        "Here I initialized a BERT tokenizer to preprocess captions and convert them into token ID's as they are of a format that the model can process. Here, we are also calculating the maximum length of a tokenized caption as we need it to define the models input size. We use tqdm library here inorder to show a progress bar during its search of all 40k captions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVfso05PaRu5",
        "outputId": "ed6db0cb-ca06-4240-f291-afdb2d5832d0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 40455/40455 [00:13<00:00, 2976.77it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Maximum sequence length in the dataset = 43\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "max_seq_len = max([len(tokenizer(caption).input_ids) for caption in tqdm(captions)])\n",
        "print(f\"Maximum sequence length in the dataset = {max_seq_len}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4E1RrhZGg0Mh"
      },
      "source": [
        "## Data splitting and assigning indeices\n",
        "\n",
        "This below cell splits the dataset indices into training, validation, and test sets based on the specified split ratios.It also checks to ensure no overlaping between different indices inorder to prevent data leakage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwL3JFo3aZLO"
      },
      "outputs": [],
      "source": [
        "# get the size of train, val and test sets\n",
        "n_train = int(n_data * SPLIT_RATIO['train'])\n",
        "n_val = int(n_data * SPLIT_RATIO['val'])\n",
        "n_test = n_data - (n_train + n_val)\n",
        "\n",
        "# permute all indices\n",
        "indices = np.random.permutation(n_data)\n",
        "\n",
        "# get train, val and test set indices\n",
        "train_indices = indices[:n_train]\n",
        "val_indices = indices[n_train:(n_train + n_val)]\n",
        "test_indices = indices[(n_train + n_val):]\n",
        "\n",
        "assert len(np.intersect1d(train_indices, val_indices)) == 0\n",
        "assert len(np.intersect1d(train_indices, test_indices)) == 0\n",
        "assert len(np.intersect1d(val_indices, test_indices)) == 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RS1CSOjRlKrM"
      },
      "source": [
        "## Splitting Data based on Obtained Indices\n",
        "Here, the get_split function takes images,captions and index as input and returns tuples of images and corresponding captions for each set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Q0c3ba4aaBI",
        "outputId": "55ef9c14-245c-4cb1-d2e8-e289690cb39d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 24273 train data, 8091 validation data and 8091 test data.\n"
          ]
        }
      ],
      "source": [
        "def get_split(images, captions, indices):\n",
        "    return zip(*[(images[idx], captions[idx]) for idx in indices])\n",
        "\n",
        "train_images, train_captions = get_split(images, captions, train_indices)\n",
        "val_images, val_captions = get_split(images, captions, val_indices)\n",
        "test_images, test_captions = get_split(images, captions, test_indices)\n",
        "\n",
        "print(f\"There are {n_train} train data, {n_val} validation data and {n_test} test data.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4X83KgQSzS1Y"
      },
      "source": [
        "## Initializing feature extractor\n",
        "The following code initializes a feature extractor using a pre-trained Vision Transformer (ViT) model. It creates the feature extractor with specific weights and designates a particular layer for feature extraction. By freezing the model parameters, the code ensures these parameters remain unchanged during training, focusing updates only on subsequent parts of the captioning model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wb0loiJRbCB4",
        "outputId": "aedb1c77-851d-42e0-e87b-4e0cda2d7c26"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\" to /root/.cache/torch/hub/checkpoints/vit_b_16-c867db91.pth\n",
            "100%|██████████| 330M/330M [00:06<00:00, 53.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "def initialize_feature_extractor(base_model, weights, feat_layer, device):\n",
        "    # initialize model\n",
        "    feature_extractor = torchvision.models.feature_extraction.create_feature_extractor(\n",
        "        BASE_MODEL(weights=WEIGHTS),\n",
        "        [FEAT_LAYER],\n",
        "    ).to(device)\n",
        "    # freeze params\n",
        "    for param in feature_extractor.parameters():\n",
        "        param.requires_grad = False\n",
        "    # set model to evaluation mode\n",
        "    feature_extractor = feature_extractor.eval()\n",
        "\n",
        "    # initialize image transformations\n",
        "    transforms = WEIGHTS.transforms()\n",
        "    return feature_extractor, transforms\n",
        "\n",
        "feature_extractor, transforms = initialize_feature_extractor(BASE_MODEL, WEIGHTS, FEAT_LAYER, device=DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doSUlGiv3sr9"
      },
      "source": [
        "## Padding Dataset for Image Captioning\n",
        "The FlickrDset class written below images, captions, transforms, and a tokenizer as inputs. The __getitem__ method loads and transforms an image, tokenizes the corresponding caption into input (y0) and target (y1) sequences for the model. The my_pad_sequence function ensures that the sequences in a batch are padded to the same length, allowing efficient batch processing. Specifically, my_pad_sequence stacks images into a tensor and pads caption sequences (y0 and y1) to have uniform lengths using PyTorch pad_sequence function, preparing data for model training or evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HczQablibQ8a"
      },
      "outputs": [],
      "source": [
        "class FlickrDset(Dataset):\n",
        "    def __init__(self, images, captions, transforms, tokenizer):\n",
        "        super().__init__()\n",
        "        self.images = images\n",
        "        self.captions = captions\n",
        "        self.transforms = transforms\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def transform_image(self, img):\n",
        "        return self.transforms(img)\n",
        "\n",
        "    def load_image(self, fp):\n",
        "        img = Image.open(os.path.join(IMAGES_DIR, fp)).convert(\"RGB\")\n",
        "        img_transformed = self.transform_image(img)\n",
        "        return img_transformed\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.load_image(self.images[idx])\n",
        "        y0 = torch.tensor(self.tokenizer(self.captions[idx]).input_ids, dtype=torch.long)\n",
        "        y1 =torch.tensor(self.tokenizer(self.captions[idx]).input_ids[1:] + [0], dtype=torch.long)\n",
        "        return x, y0, y1\n",
        "\n",
        "def my_pad_sequence(data):\n",
        "    x, y0, y1 = zip(*data)\n",
        "    x = torch.stack(x)\n",
        "    y0 = torch.nn.utils.rnn.pad_sequence(y0, batch_first=True)\n",
        "    y1 = torch.nn.utils.rnn.pad_sequence(y1, batch_first=True)\n",
        "    return (x, y0, y1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Lq0KU-R5Ej5"
      },
      "source": [
        "## Initializing Datasets and Dataloaders\n",
        "This code block initializes datasets and data loaders for training, validation, and testing. It creates a FlickrDset class for each dataset split, then wraps these in DataLoader objects. The train_iterator shuffles and drops the last batch for efficient training, while val_iterator and test_iterator do not shuffle and include all batches. The collate_fn=my_pad_sequence argument ensures sequences are padded to the same length within each batch. An additional inference_iterator is set up for single-image inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4MOGV4McDvm"
      },
      "outputs": [],
      "source": [
        "# initialize train, val and test datasets\n",
        "train_dset = FlickrDset(train_images, train_captions, transforms, tokenizer)\n",
        "val_dset = FlickrDset(val_images, val_captions, transforms, tokenizer)\n",
        "test_dset = FlickrDset(test_images, test_captions, transforms, tokenizer)\n",
        "\n",
        "# initialize train, val and test iterators\n",
        "train_iterator = DataLoader(train_dset, BATCH_SIZE, shuffle=True, drop_last=True, collate_fn=my_pad_sequence, pin_memory=True)\n",
        "val_iterator = DataLoader(val_dset, BATCH_SIZE, shuffle=False, drop_last=False, collate_fn=my_pad_sequence, pin_memory=True)\n",
        "test_iterator = DataLoader(test_dset, BATCH_SIZE, shuffle=False, drop_last=False, collate_fn=my_pad_sequence, pin_memory=True)\n",
        "\n",
        "# initialize test iterator for inference (batch_size=1)\n",
        "inference_iterator = DataLoader(test_dset, 1, shuffle=False, drop_last=False, pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFPbg-XO8Bkw"
      },
      "source": [
        "## Transformer-based Caption Generation\n",
        "The Transformer decoder processes the embedded input tokens, attending to image features. If is_causal is True, it employs a causal mask to prevent attending to future tokens during training. Finally, a linear layer projects decoder outputs to vocabulary space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Edn_H0MscEkS"
      },
      "outputs": [],
      "source": [
        "class CaptionGenerator(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, max_seq_len, nhead, num_layers):\n",
        "        super().__init__()\n",
        "        self.word_embed_lookup = nn.Embedding(vocab_size, embed_size)\n",
        "        self.pos_embed_lookup = nn.Embedding(max_seq_len, embed_size)\n",
        "\n",
        "        decoder_layer = torch.nn.TransformerDecoderLayer(embed_size, nhead, batch_first=True)\n",
        "        self.decoder = torch.nn.TransformerDecoder(decoder_layer, num_layers)\n",
        "\n",
        "        self.fc = nn.Linear(embed_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, image_embed, is_causal):\n",
        "\n",
        "        seq_len = x.shape[1]\n",
        "\n",
        "        word_embed = self.word_embed_lookup(x)\n",
        "        pos_embed = self.pos_embed_lookup(torch.arange(seq_len).to(x.device))\n",
        "        x_embed = word_embed + pos_embed\n",
        "\n",
        "        if is_causal:\n",
        "            tgt_mask = nn.Transformer.generate_square_subsequent_mask(seq_len).to(x.device)\n",
        "        else: tgt_mask=None\n",
        "\n",
        "        x = self.decoder(\n",
        "            tgt=x_embed,\n",
        "            memory=image_embed,\n",
        "            tgt_mask=tgt_mask\n",
        "        )\n",
        "        x = self.fc(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wna2D3Km8z_X"
      },
      "source": [
        "## Using loss as Evaluation Metric\n",
        "This function computes the cross-entropy loss given model logits and ground truth labels. It first reshapes the logits tensor to a 2D shape (batch size * sequence length, number of classes) and flattens the ground truth labels. Then, it computes the cross-entropy loss between the reshaped logits and flattened labels, ignoring padding tokens with index 0. Finally, it returns the computed loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iy8RR_dYcVuE"
      },
      "outputs": [],
      "source": [
        "def compute_loss(logits, y):\n",
        "    B, S, C = logits.shape\n",
        "    loss = F.cross_entropy(logits.view(B*S, C), y.view(-1), ignore_index=0)\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xAfMpQ69ffQ"
      },
      "source": [
        "## Using loss for fine tuning of hyperparameters\n",
        "This function batch_loop takes the feature extractor, input data (images and captions), and a boolean flag as input indicating whether to use a causal mask during decoding. It performs the following steps:\n",
        "\n",
        "*   Passes the input images through the feature extractor to obtain image embeddings.\n",
        "\n",
        "*   Feeds the image embeddings and input captions to the model to generate logits (raw predictions) for the next tokens in the captions.\n",
        "\n",
        "*   Calculates the loss using the computed logits and the ground truth next tokens (targets) using the compute_loss function.\n",
        "\n",
        "This function essentially makes a single forward pass through the model and computes the corresponding loss, which is then used for backpropagation and parameter updates during training.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HE_UtCQccYlv"
      },
      "outputs": [],
      "source": [
        "def batch_loop(model, feature_extractor, x, y0, y1, is_causal):\n",
        "    x, y0, y1 = x.to(DEVICE), y0.to(DEVICE), y1.to(DEVICE)\n",
        "    x_embed = feature_extractor(x)[FEAT_LAYER].unsqueeze(1)\n",
        "    logits = model(y0, x_embed, is_causal=is_causal)\n",
        "    loss = compute_loss(logits, y1)\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HqMeNg0Arao"
      },
      "source": [
        "## Training Epoch Function\n",
        "This function performs one epoch of training for the caption generation model. It iterates over the training data batches, computing the loss for each batch using the batch_loop function. The model parameters are updated via backpropagation using the optimizer. It accumulates the total loss across batches and calculates the average loss per batch. The model is set to training mode, while the feature extractor is in evaluation mode to prevent its parameters from being updated. Finally, it returns the average loss for the epoch, which is used for monitoring training progress."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myzmiTEkce1G"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, feature_extractor, iterator, optimizer):\n",
        "    model.train()\n",
        "    feature_extractor.eval()\n",
        "    loss_sum = 0.0\n",
        "    for iter_idx, (x, y0, y1) in tqdm(enumerate(iterator), total=len(iterator), desc=\"Training Progress\"):\n",
        "        # compute batch loss\n",
        "        loss = batch_loop(model, feature_extractor, x, y0, y1, is_causal=True)\n",
        "        # update model\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # add loss to total sum\n",
        "        loss_sum += loss.item()\n",
        "\n",
        "    # compute average epoch loss\n",
        "    n_batches = len(iterator)\n",
        "    loss_avg = loss_sum / n_batches\n",
        "    return loss_avg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lztiql77BG_3"
      },
      "source": [
        "## Evaluation Function\n",
        "This function evaluates the performance of the caption generation model on a validation or test dataset. It iterates over the data batches, computing the loss for each batch using the batch_loop function. This function provides insight into how well the model generalizes to unseen data and is useful for model selection and hyperparameter tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kREK1aRQciky"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, feature_extractor, iterator):\n",
        "    model.eval()\n",
        "    feature_extractor.eval()\n",
        "    loss_sum = 0.0\n",
        "    with torch.inference_mode():\n",
        "        # iterate over batches\n",
        "        for iter_idx, (x, y0, y1) in tqdm(enumerate(iterator), total=len(iterator), desc=\"Evaluation Progress\"):\n",
        "            loss = batch_loop(model, feature_extractor, x, y0, y1, is_causal=True)\n",
        "            # add loss to total sum\n",
        "            loss_sum += loss.item()\n",
        "\n",
        "    # compute average epoch loss\n",
        "    n_batches = len(iterator)\n",
        "    loss_avg = loss_sum / n_batches\n",
        "    return loss_avg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvzF0QqeBrh9"
      },
      "source": [
        "## Text Generation Function\n",
        "This function generates captions for input images.It generates tokens for the caption, conditioning on the image features and previously generated tokens. At each step, it samples the next token from the probability distribution over the vocabulary generated by the model. The process continues until either the maximum length is reached or the special [SEP] token is generated, indicating the end of the caption."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgWXGnNRcjTi"
      },
      "outputs": [],
      "source": [
        "def generate(model, feature_extractor, x, tokenizer, temp, max_length, device):\n",
        "    model.eval()\n",
        "    feature_extractor.eval()\n",
        "    with torch.inference_mode():\n",
        "        x_embed = feature_extractor(x)[FEAT_LAYER].unsqueeze(1)\n",
        "        input_tokens = torch.tensor([tokenizer.convert_tokens_to_ids(['[CLS]'])], dtype=torch.long).to(device)\n",
        "\n",
        "        generated_token_ids = []\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            logits = model(input_tokens, x_embed, is_causal=False)[:,-1,:]\n",
        "            next_token = torch.multinomial(logits.view(1, -1).div(temp).exp(), num_samples=1)\n",
        "            input_tokens = torch.cat((input_tokens, next_token), dim=1)\n",
        "            if tokenizer.decode([next_token.item()]) == '[SEP]':\n",
        "                break\n",
        "\n",
        "            generated_token_ids.append(next_token.item())\n",
        "\n",
        "        return generated_token_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQjAJjnjCti_"
      },
      "source": [
        "## Training, Evaluation and Overfitting prevention of Model\n",
        "The caption generation model is initialized with specified hyperparameters such as vocabulary size, embedding size, maximum sequence length, number of attention heads, and number of layers. An AdamW optimizer is also initialized to optimize the model parameters.\n",
        "\n",
        " The model is trained for a specified number of epochs (N_EPOCHS). During each epoch, the model is trained on the training dataset using the train_epoch function. After training, the model performance is evaluated on the validation dataset using the evaluate function to compute the validation loss (val_loss).\n",
        "\n",
        "**Best Model Selection:** If the validation loss decreases, indicating an improvement in performance, the current model parameters are saved as the best model. The best model's parameters are saved to the file specified by BEST_MODEL_FILEPATH. Additionally, the model's performance is evaluated on the test dataset, and the generated captions for a few test images are displayed.\n",
        "\n",
        "**Early Stopping:** If the validation loss does not improve for a certain number of epochs (EARLY_STOPPING_STEP_SIZE), the training process is terminated early to prevent overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYoSiA5xcnSx",
        "outputId": "b4634d4b-465c-49f7-d092-10c4b5b7c27a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress: 100%|██████████| 189/189 [6:06:59<00:00, 116.50s/it]\n",
            "Evaluation Progress:  22%|██▏       | 14/64 [22:10<1:21:59, 98.38s/it]"
          ]
        }
      ],
      "source": [
        "# initialize model\n",
        "model = CaptionGenerator(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    embed_size=EMBED_SIZE,\n",
        "    max_seq_len=max_seq_len,\n",
        "    nhead=NHEAD,\n",
        "    num_layers=NUM_LAYERS,\n",
        ").to(DEVICE)\n",
        "\n",
        "# initialize optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "# initialize best val loss and best model streak count\n",
        "best_val_loss = float('inf')\n",
        "best_streak_count = 0\n",
        "\n",
        "# iterate over epochs\n",
        "for epoch_idx in range(1, N_EPOCHS+1):\n",
        "    # train\n",
        "    train_loss = train_epoch(model, feature_extractor, train_iterator, optimizer)\n",
        "    # evaluate\n",
        "    val_loss = evaluate(model, feature_extractor, val_iterator)\n",
        "    test_loss = evaluate(model, feature_extractor, test_iterator)\n",
        "\n",
        "    # print losses\n",
        "    print(f\"Epoch {epoch_idx:02} - Train loss = {train_loss:.4f}\\tVal loss = {val_loss:.4f}\\tTest loss = {test_loss:.4f}\")\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        # save the curent model's parameters as the best model parameters\n",
        "        torch.save(model.state_dict(), BEST_MODEL_FILEPATH)\n",
        "        # replace the best test loss with the current best loss\n",
        "        best_val_loss = val_loss\n",
        "        # reset early stoppping counter\n",
        "        best_streak_count = 0\n",
        "        # display info\n",
        "        print(f'The best model is found and saved. Current best val loss = {best_val_loss:.3f}\\n')\n",
        "\n",
        "        # ------------------------------------------------------\n",
        "        ct = 0\n",
        "\n",
        "        for x, y0, y1 in inference_iterator:\n",
        "            x = x.to(DEVICE)\n",
        "            # generate caption\n",
        "            generated_token_ids = generate(model, feature_extractor, x, tokenizer, temp=1.0, max_length=max_seq_len, device=DEVICE)\n",
        "            caption_generated = tokenizer.decode(generated_token_ids)\n",
        "            # get groundtruth caption\n",
        "            caption_gt = tokenizer.decode(y0[0,1:-1].cpu().numpy().tolist())\n",
        "            # show image\n",
        "            img = Image.open(os.path.join(IMAGES_DIR, test_images[ct])).convert(\"RGB\")\n",
        "            plt.imshow(img)\n",
        "            plt.show()\n",
        "            # print generated and groundtruth captions\n",
        "            print(\"True:\\n\", caption_gt)\n",
        "            print(\"Generated:\\n\", caption_generated)\n",
        "            ct += 1\n",
        "            if ct > 5: break\n",
        "        # ------------------------------------------------------\n",
        "    else:\n",
        "        # update early stoppping counter\n",
        "        best_streak_count += 1\n",
        "\n",
        "    # check early stopping condition\n",
        "    if best_streak_count == EARLY_STOPPING_STEP_SIZE:\n",
        "        print(f\"A better model has not been found in the last {EARLY_STOPPING_STEP_SIZE} epochs. Early stopping...\")\n",
        "        break\n",
        "\n",
        "    print(\"--------------\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "if6Li5LtDtEY"
      },
      "source": [
        "# **Summary Report**\n",
        "## **Model Overview**\n",
        "The problem statement stated to develop a caption generation model. In the model, I used a combination of a Vision Transformer (ViT) as the feature extractor and a Transformer-based decoder for generating captions. The model uses a  pre-trained ViT to extract image features, which are then sent to a Transformer decoder that generates captions token-by-token.\n",
        "\n",
        "## **Model Performance**\n",
        "The model was trained on a dataset of images and their corresponding captions. Performance metrics include the average training loss, validation loss, and test loss over multiple epochs(runs).Evaluation was performed by generating captions for a subset of test images and comparing them with the ground truth captions(The actual caption).\n",
        "\n",
        "## **Challenges Faced**\n",
        "1.   **Overfitting:** When I initially ran the model it generated huge loss when compared with the truth caption of Evatuation data. This led me to use the early stopping technique\n",
        "\n",
        "2.   **Data Handling:** Uploading the Dataset was itself a huge issue and took a lot of time. I initially wanted to put a repositry link but when ever i pushed the files, it truncated the number of images to 100 automatically. This forced me to use drive.\n",
        "\n",
        "3.   **Variation in caption Length:** When issuing tokens to captions, the variation in length was initially a hassle, this forced me to learn and use padding techniques inorder to make the string length same.\n",
        "\n",
        "4.   **Training Time:** While I used ViT over a CNN for better results in the model. This resulted in huge training time. So, everytime i wanted to check my model, it took a lot of time to run.\n",
        "\n",
        "## **Methods used to improve the model**\n",
        "\n",
        "\n",
        "1.   **Early Stopping:** Early stopping was implemented to halt training when the validation loss did not improve for a specified number of epochs. This prevented the model from overfitting to the training data.\n",
        "\n",
        "2.   **Freezing Feature Extractor:** The ViT feature extractor's parameters were frozen during training to use its pre-trained weights and simplifies the model by reducing the number of trainable parameters, making it easier to optimize and less prone to overfitting\n",
        "\n",
        "3. **Batch Padding:** Custom padding for batch sequences made sure that sequences were padded to the same length within each batch.This reduces th etime taken for training by the model\n",
        "\n",
        "4. **AdamW Optimizer:** I used AdamW optimizer with a learning rate scheduler to train the model faster and better. It helped adjust how quickly the model learns from data, making sure it converges to the best solution efficiently. This way, my model learns more effectively and improves its performance over time.\n",
        "\n",
        "## **References**\n",
        "\n",
        "### **Vision Transformers(ViT):**\n",
        "\n",
        "*   Dosovitskiy, A., et al. (2020). An image is worth 16x16 words: Transformers for image recognition at scale.\n",
        "\n",
        "*   Touvron, H., et al. (2021). Training data-efficient image transformers & distillation through attention. In International Conference on Machine Learning (pp. 10347-10357). PMLR.\n",
        "\n",
        "*   https://github.com/facebookresearch/detectron2\n",
        "\n",
        "*   https://github.com/pytorch/vision\n",
        "\n",
        "### **Image Captioning:**\n",
        "\n",
        "*    Vinyals, O., Toshev, A., Bengio, S., & Erhan, D. (2015). Show and tell: A neural image caption generator. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 3156-3164).\n",
        "\n",
        "*    Xu, K., et al. (2015). Show, attend and tell: Neural image caption generation with visual attention. In International Conference on Machine Learning (pp. 2048-2057).\n",
        "\n",
        "*    Anderson, P., et al. (2018). Bottom-up and top-down attention for image captioning and visual question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 6077-6086).\n",
        "\n",
        "*    https://github.com/tensorflow/models/tree/master/research/im2txt\n",
        "\n",
        "*    https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning\n",
        "\n",
        "### **Transformers in NLP:**\n",
        "\n",
        "*    Vaswani, A., et al. (2017). Attention is all you need. In Advances in Neural Information Processing Systems (pp. 5998-6008).\n",
        "\n",
        "*    Devlin, J., et al. (2018). BERT: Pre-training of deep bidirectional transformers for language understanding.\n",
        "\n",
        "*    https://github.com/huggingface/transformers\n",
        "\n",
        "*    https://github.com/allenai/allennlp\n",
        "\n",
        "*    \n",
        "\n",
        "### **Combining Vision and Language Models:**\n",
        "\n",
        "*    Li, L. H., et al. (2020). Oscar: Object-semantics aligned pre-training for vision-language tasks. In European Conference on Computer Vision (pp. 121-137).\n",
        "\n",
        "*    Kim, J. H., et al. (2018). Bilinear attention networks. In Advances in Neural Information Processing Systems (pp. 1564-1574).\n",
        "\n",
        "*    Chen, Y. C., et al. (2020). UNITER: UNiversal Image-TExt Representation Learning. In European Conference on Computer Vision (pp. 104-120).\n",
        "\n",
        "*    https://github.com/jiasenlu/vilbert_beta\n",
        "\n",
        "### **Early Stopping**\n",
        "\n",
        "*    rechelt, L. (1998). Early stopping-but when? In Neural Networks: Tricks of the Trade (pp. 55-69).\n",
        "\n",
        "*    Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep learning. MIT Press.\n",
        "\n",
        "*    https://github.com/Bjarten/early-stopping-pytorch\n",
        "\n",
        "*    https://github.com/keras-team/keras-tuner\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmcKB9dDqVqP"
      },
      "source": [
        "# **Reflections**\n",
        "## **Choices Made**\n",
        "\n",
        "*   **Use of ViT over CNN:**\n",
        "\n",
        " The Vision Transformer is particularly useful when large-scale datasets are available for training, as it can capture long-range dependencies and global context more effectively than traditional CNNs. In this model I used ViT for its capability to extract high-level features from images, which are then used for generating captions.\n",
        "\n",
        "* **Use of Loss as an Evaluation Parameter over BLEU/METOR/ROGUE:**\n",
        "\n",
        " In this code, I used loss as an evaluation metric over BLEU due to its suitability. Loss measures the difference between predicted and actual values, this directly reflects the model's ability to minimize errors during training, guiding parameter updates towards an ideal value. Conversely, BLEU (Bilingual Evaluation Understudy) is a metric commonly used in natural language processing for evaluating machine translation tasks. However, it relies on human-generated reference texts, making it less suitable particularly in tasks like image captioning where generating contextually relevant captions is essential.\n",
        "\n",
        "* **Utilizing Pre-Trained embeddings over training embeddings from scratch:**\n",
        "\n",
        " Utilizing pretrained word embeddings (such as Word2Vec or GloVe) instead of training embeddings from scratch. Pretrained embeddings capture semantic relationships between words and help in improving the model's performance, especially when training data is limited.\n",
        "\n",
        "\n",
        " ## **Potential Improvements**\n",
        "\n",
        "* **Fine-tuning the Feature Extractor:** Instead of freezing the\n",
        "  parameters of the ViT feature extractor, fine-tuning them along with the caption generator could allow the model to adapt better to the specific task and potentially improve performance.\n",
        "\n",
        "* **Data Augmentation:** Applying data augmentation techniques to\n",
        "  the image data, such as random cropping, rotation, or flipping, could increase the diversity of training examples and help the model generalize better to unseen data.\n",
        "\n",
        "* **Ensemble Learning:** Training multiple caption generator models with different architectures or initializations and combining their predictions through ensemble learning techniques could lead to improved generalization and performance.\n",
        "\n",
        "* **Attention Mechanism Variants:** Exploring different variants\n",
        "  of the transformer attention mechanism, such as self-attention with relative positional encoding or multi-head attention, could potentially capture more complex relationships in the data and enhance model performance."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "12ElCt5VKjluj3P4rklgaekvjvKVc2M_c",
      "authorship_tag": "ABX9TyO2W/yVko2w5e+J/IDIldHV",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}